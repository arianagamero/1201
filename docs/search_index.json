[
["index.html", "Stat 1201 Resources Syllabus Course material and assignments Zoom info Instructor Teaching assistant Course description Resources and readings Grading procedures Problem Sets Tests R Academic Integrity Getting help Course content", " Stat 1201 Resources Joyce Robbins 2020-09-07 Syllabus Columbia University Calculus-Based Introduction to Statistics Statistics UN1201, Fall 2020 Tuesday / Thursday 11:40am - 12:55pm online only Course material and assignments (These items will be moved to CourseWorks once the change of program period is over.) Zoom info You are expected to attend Zoom lectures in real time. Recordings will be available primarily for students in different time zones or to make up an occasional missed lecture. During class, it is your choice whether to turn your video on or not; please however add a profile picture to your Zoom account to help us feel like a class with faces and not a list of names. You are strongly encouraged to turn your video on during small group breakout rooms, and you will need to turn your video on during exams (more details will be provided later on the specifics of how this will work.) Instructor Joyce Robbins: jtr13@columbia.edu tel: 212-853-1391 Office hours: TBD Teaching assistant Haolin Zou, hz2574@columbia.edu Office hours: TBD Course description This introductory course is designed for students who desire a strong grounding in statistical concepts, and has a greater degree of mathematical rigor than STAT UN1101. Topics include: graphical and numerical summaries of data; the normal distribution, probability, random variables, and sampling distributions; estimation and hypothesis testing for means and proportions; comparing two populations; two-way contingency tables; introduction to linear regression. This course serves as the pre-requisite for ECON UN3412. Resources and readings Required text Devore, Jay L. 2016. Probability and Statistics for Engineering and the Sciences. 9th edition. ISBN:9781305251809 You need the 9th edition for the exercises; you can use earlier editions for the content. Recommended text Gonick, Larry &amp; Woollcott Smith. 2005. The Cartoon Guide to Statistics. ISBN:9780062731029 Any edition is fine. Other materials For the tests and final, you need a non-graphing calculator (one line display). Statistical tables will be provided. You will also need a phone scanning app (such as CamScanner Free) or access to a scanner to create PDFs to turn in problem sets on CourseWorks. Grading procedures Grades will be determined as follows: Homework + class problem 20% Test #1 20% Test #2 20% Final 40% Grading scale: (Note: Do not rely on any averages that appear in CourseWorks since they may not be accurate for various reasons.) 97% + A+ 80% to 83% B- 93% to 97% A 75% to 80% C+ 90% to 93% A- 70% to 75% C 87% to 90% B+ 60% to 70% C- 83% to 86% B below 60% F Note: The tests and final exam are curved on an individual basis, meaning one test may have a curve while another does not. The grades posted on CourseWorks are the grades AFTER the curve. There is no additional curve in the process of computing final averages at the end of the semester. Raw scores are available on the paper tests which will be handed back in class. If you are not in class when tests are handed back, you may pick up your exam the following class. Problem Sets Unless otherwise indicated, assignments must be submitted on CourseWorks as a *single pdf file (not as an image file or file.) Assuming you don’t have access to a scanner, use a phone app such as CamScanner Free for this purpose. (If you have another app that works well please let me know!) Homework must be submitted by 11:59pm on the due date for full credit. Late assignments will only be accepted for the first 24 hours after the assignment is due. You are entitled to one “free” late assignment, submitted within 24 hours of the due date, without penalty. After that, 4 points will be deducted for late assignments submitted within 24 hours of the due date. After the 24 hour period, you will receive a zero for the assignment. No exceptions will be made unless I receive a note from an academic dean detailing unusual circumstances. If you submit more than one assignment, the last one will be counted. Be sure that homework is clearly labeled and legible. Note that not all homework questions are graded so be sure to check all of your work against the posted solutions. (In short, a 20/20 doesn’t necessarily mean that everything is right.) Tentative problem set due dates. Tues, Sept 15 (if you are admitted into the class after Friday, Sept 11, you can take until Mon, Sep 21 to submit this assignment) Thurs, Sept 24 Tues, Oct 6 Tues, Oct 20 Thurs, Oct 29 Thurs, Nov 12 Tues, Dec 1 Thurs, Dec 10 If there are any changes, they will be announced on CourseWorks. (If there are discrepancies between what is listed here and on CourseWorks, CourseWorks takes precedence.) All problem sets count; in other words, no problem set grades are dropped. Tests More details will be provided later on the logistics of taking exams via Zoom. Test 1: Thurs, Oct 8 Test 2: Tues, Nov 17 Allowable materials on tests include pens, pencils, erasers, a hand-held NON-GRAPHING calculator (no graphics display), and handwritten notes. Photocopies of notes are NOT acceptable. The number of double-sided sheets you can bring depends on the test: Test #1: one double-sided sheet Test #2: two double-sided sheets Final: three double-sided sheets Save your notes so that you can reuse them (if you wish) on subsequent tests. Make-up tests will only be administered in cases of documented emergencies. All tests are cumulative but weighted more toward new material. For the date of the Final, see https://ssol.columbia.edu/ – click “Exam List” and then “Projected University Examination Schedule”. (The date of the final is set by the University, not by me.) R Information on installing R and getting started is provided in the next chapter. Note that all of the coding related parts of the class are optional and will not appear on the tests. Academic Integrity You are encouraged to discuss homework strategies with your classmates and work through examples, but you may not discuss particular homework problems, nor copy all or part of solutions from anywhere. In short, the homework you submit must be your own work. Plagiarism or any other breach of academic integrity will not be tolerated and will result in disciplinary action. For more information please refer to the The Columbia University Undergraduate Guide to Academic Integrity: http://www.college.columbia.edu/academics/academicintegrity Accommodations for students registered with Disability Services In order to receive disability-related academic accommodations for this course, students must first be registered with their school Disability Services (DS) office. Detailed information is available online for both the Columbia and Barnard registration processes. Refer to the appropriate website for information regarding deadlines, disability documentation requirements, and drop-in hours (Columbia) / intake session (Barnard). For this course, students registered with the Columbia office are not required to have testing forms or accommodation letters signed by faculty. However, students must do the following: The Instructor section of the form has already been completed and does not need to be signed by the professor. The student must complete the Student section of the form and submit the form to Disability Services. Master forms are available in the Disability Services office or online: https://health.columbia.edu/services/testing-accommodations Students registered with the Barnard office should follow standard procedures, which require the professor’s signature. The forms should be brought to class, not sent electronically. Getting help There are numerous opportunities to get assistance with the class material: Ask questions during class. Email me questions after class and I will answer during the next lecture. Ask questions after class (I will stay after every class if students have questions.) Attend a Statistics Help Room session where tutors answer questions for students in introductory level statistics classes. I will announce when the Help Room opens for the semester; generally the 2nd or 3rd week of classes. Hours and locations are posted here: http://stat.columbia.edu/help-room/ (Once the Help Room opens, the page will be updated to the current semester.) Attend TA office hours: see above for times and locations Course content The following topics will be covered in the order below. For more detailed information on material covered within each section, please consult 1201.info Descriptive Statistics (1.1-1.4) Probability (2.1-2.3, 2.5) Discrete random variables (3.1-3.5) Continuous random variables (4.1-4.2) Normal and other continuous distributions (4.3-4.4) The sampling distribution of a mean (5.3-5.5) Point estimation (6.1) Confidence intervals (7.1-7.3) Hypothesis testing – one sample (8.1-8.5) Hypothesis testing – two samples (9.1-9.4) Conditional probability (2.4) Joint probability (5.1) Chi squared test (14.3) Covariance, correlation (5.2, 12.5) Linear regression (12.1-12.2, 13.1-13.2) "],
["introduction.html", "Introduction General study tips Installing R Getting Started with R Contact License", " Introduction This site contains supplemental materials for Stat 1201, mainly: 1) clarifications on which sections we cover in the textbook (Devore, Probability and Statistics for Engineering and the Sciences 9th edition), 2) R code, and 3) links to helpful resources online. It is not in any way a substitute for materials available in CourseWorks. If you find additional online resources that are helpful to this class, please create an issue or send me an email and I’ll add them to this resource. Let me know as well if you find any typos or other mistakes. Note that while you’re encouraged to look ahead, be sure to circle back to those sections when they’re covered in class since content may be added or modified slightly. General study tips The website for the book Make It Stick offers a summary of the experimentally tested study strategies. The tl;dr is: working out problems is better than reviewing notes / textbook doing mixed reviews is better than focusing on one type of problem at a time learning is hard work; if it seems too easy your study strategy might not be the most effective making mistakes and learning from them is a useful strategy (don’t wait until you’ve mastered all of the examples to try a problem) You’ve likely heard a lot of these ideas before, but it’s worth really thinking about them and putting them into practice. My advice: As you’re reading the textbook or working on a problem set, keep a list of questions. Challenge yourself by thinking about how the problem would differ if you changed the setup. Try creating your own questions and solving them. Try solving problems in multiple ways. Learn from a variety of sources: class, textbook, Cartoon Guide, etc. If you find differences, ask. Installing R You will need to install two applications: R and RStudio: R – the programming language itself – is available here: https://cran.r-project.org RStudio – an integrated development environment (IDE) which makes it much easier to use R. It is optional but highly recommended. This is the app you will open to use R. https://www.rstudio.com/products/rstudio/download/#download Getting Started with R To get oriented, read and try the examples in Chapter 1 of Introduction to R Quick intro to RStudio Console Working in the console pane is similar to a using a calculator: each line of code is executed when you press enter. Note that your work is not saved with this approach. Assigning a variable Drawing a stem and leaf plot Saving your code as an .R file Video Screenshot Saving with this method saves only the code, not the output. Below are two methods for creating .html documents that contain both code and output: Convert .R file to .html Video Screenshots Save code and text as .Rmd, convert to .html This format is called “Rmarkdown” since it combines R and markdown. Markdown is a simple format that is converted to .html For a short summary of markdown, click Help and then “Markdown Quick Reference” in RStudio. Video Using R as a calculator Basic operations: 3 + 4 ## [1] 7 3 - 4 ## [1] -1 3 * 4 ## [1] 12 3 / 4 ## [1] 0.75 3^4 ## [1] 81 Working with vectors: x &lt;- 1:5 x ## [1] 1 2 3 4 5 sum(x) ## [1] 15 cumsum(x) ## [1] 1 3 6 10 15 px &lt;- x*.05 + .05 px ## [1] 0.10 0.15 0.20 0.25 0.30 Expected value: x*px ## [1] 0.1 0.3 0.6 1.0 1.5 sum(x*px) ## [1] 3.5 Contact Joyce Robbins: Columbia Profile GitHub License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. "],
["ch-1-descriptive-statistics.html", "Ch. 1 Descriptive Statistics 1.1 Populations, Samples, and Processes 1.2 Pictorial and Tabular Methods in Descriptive Statistics 1.3 Measures of location 1.4 Measures of variability Practice Exercises", " Ch. 1 Descriptive Statistics Sections covered: all 1.1 Populations, Samples, and Processes 1.2 Pictorial and Tabular Methods in Descriptive Statistics Skip: Example 1.7, p. 15 (double-digit leaves) Skip: “Dotplots,” pp. 15-16 R Stem-and-leaf display: prices &lt;- c(379, 425, 450, 450, 499, 529, 535, 535, 545, 599, 665, 675, 699, 699, 725, 725, 745, 799) stem(prices) ## ## The decimal point is 2 digit(s) to the right of the | ## ## 3 | 8 ## 4 | 355 ## 5 | 03445 ## 6 | 078 ## 7 | 00335 ## 8 | 0 Note that histograms are drawn with unbinned data. R does the binning in the process of drawing the histogram. Frequency histogram: prices &lt;- c(379, 425, 450, 450, 499, 529, 535, 535, 545, 599, 665, 675, 699, 699, 725, 725, 745, 799) hist(prices) hist(prices, breaks = c(300, 400, 500, 600, 700, 800), col = &quot;lightblue&quot;) Density histogram: prices &lt;- c(379, 425, 450, 450, 499, 529, 535, 535, 545, 599, 665, 675, 699, 699, 725, 725, 745, 799) hist(prices, freq = FALSE, breaks = c(300, 400, 500, 600, 700, 800), col = &quot;lightblue&quot;, las = 1) Cumulative frequency histogram For this type of histogram, we need access to the bin counts, in order to calculate the cumulative frequencies. The hist() function returns these values, if assigned to a variable: x &lt;- c(1, 1, 1, 1, 1, 5, 5, 5, 7, 7, 8) myhistdata &lt;- hist(x) myhistdata ## $breaks ## [1] 1 2 3 4 5 6 7 8 ## ## $counts ## [1] 5 0 0 3 0 2 1 ## ## $density ## [1] 0.45454545 0.00000000 0.00000000 0.27272727 0.00000000 0.18181818 ## [7] 0.09090909 ## ## $mids ## [1] 1.5 2.5 3.5 4.5 5.5 6.5 7.5 ## ## $xname ## [1] &quot;x&quot; ## ## $equidist ## [1] TRUE ## ## attr(,&quot;class&quot;) ## [1] &quot;histogram&quot; The particular information we want is $counts: myhistdata$counts ## [1] 5 0 0 3 0 2 1 The cumulative frequencies are: cumsum(myhistdata$counts) ## [1] 5 5 5 8 8 10 11 To plot them, we need to use a bar chart, not a histogram, since we already have the y-axis values: barplot(cumsum(myhistdata$counts)) Cleaned up: barplot(cumsum(myhistdata$counts), col = &quot;lightblue&quot;, space = 0, # remove gaps between bars las = 1, # make all tick mark labels horizontal ylim = c(0, 12), # make the y-axis longer names.arg = myhistdata$mids ) 1.3 Measures of location Skip: Example 1.16, p. 33 (trimmed mean) Skip: “Categorical Data and Sample Proportions,” p. 34 (We’ll return to this topic later.) R prices &lt;- c(379, 425, 450, 450, 499, 529, 535, 535, 545, 599, 665, 675, 699, 699, 725, 725, 745, 799) mean(prices) ## [1] 593.2222 median(prices) ## [1] 572 ## quartiles quantile(prices) ## 0% 25% 50% 75% 100% ## 379.0 506.5 572.0 699.0 799.0 ## trimmed mean mean(prices, trim = .1) ## 10% trimmed mean ## [1] 593.75 1.4 Measures of variability Skip: extreme outliers (p. 42) We will define outliers for boxplots to be observations that are more than 1.5 times the fourth spread from the closest fourth. They may be indicated with either a solid or open circle (in contrast to the book which uses one for mild outliers and the other for extreme outliers.) R Sample variance: prices &lt;- c(379, 425, 450, 450, 499, 529, 535, 535, 545, 599, 665, 675, 699, 699, 725, 725, 745, 799) var(prices) ## [1] 15981.48 Sample standard deviation: sqrt(var(prices)) ## [1] 126.4179 sd(prices) ## [1] 126.4179 Five number summary (min, lower-hinge, median, upper-hinge, max) fivenum(prices) ## [1] 379 499 572 699 799 Boxplots prices &lt;- c(379, 425, 450, 450, 499, 529, 535, 535, 545, 599, 665, 675, 699, 699, 725, 725, 745, 799) boxplot(prices) boxplot(prices, horizontal = TRUE) PTSD &lt;- c(10, 20, 25, 28, 31, 35, 37, 38, 38, 39, 39, 42, 46) Healthy &lt;- c(23, 39, 40, 41, 43, 47, 51, 58, 63, 66, 67, 69, 72) df &lt;- data.frame(Healthy, PTSD) boxplot(df, horizontal = TRUE) Practice Exercises Using the built-in dataset ToothGrowth in R, visualize the data and comment on the effectiveness of different functions in the context. # The first 5 rows of the data head(ToothGrowth, 5) ## len supp dose ## 1 4.2 VC 0.5 ## 2 11.5 VC 0.5 ## 3 7.3 VC 0.5 ## 4 5.8 VC 0.5 ## 5 6.4 VC 0.5 # Five number summary fivenum(ToothGrowth$len) ## [1] 4.20 12.55 19.25 25.35 33.90 # Boxplot # &#39;$&#39; extracts the column by name boxplot(ToothGrowth$len) # Stem-and-leaf Plot stem(ToothGrowth$len) ## ## The decimal point is 1 digit(s) to the right of the | ## ## 0 | 4 ## 0 | 5667789 ## 1 | 00001124 ## 1 | 55555677777899 ## 2 | 001222333344 ## 2 | 55566666667779 ## 3 | 0134 # Histogram h &lt;- hist(ToothGrowth$len) # Cumulative Histogram h$counts &lt;- cumsum(h$counts) plot(h) head(): directly see how the dataset looks; useful when the dataset is large and it’s difficult to display all rows and columns together. fivenum(): returns the minimum value, lower fourth, median, upper fourth, and maximum value boxplot(): visualizes the five number summary plus outliers. (It’s clear that the ToothGrowth data is not skewed.) stem(): compares the number of data points that fall in different bins. (Here we can see that most values are between 20 and 29.) hist(): draws a histogram – values are grouped in bins cumsum(): takes a vector and returns the cumulative sums "],
["ch-2-probability.html", "Ch. 2 Probability 2.1 Sample Spaces and Events 2.2 Axioms, Interpretations, and Properties of Probability 2.3 Counting Techniques 2.5 Independence", " Ch. 2 Probability Sections covered: 2.1, 2.2, 2.3, 2.5 (2.4 will be covered later in the semester) 2.1 Sample Spaces and Events Humor Source: https://twitter.com/bcrypt/status/1074415553266122752/photo/1 2.2 Axioms, Interpretations, and Properties of Probability 2.3 Counting Techniques R Factorial factorial(5) ## [1] 120 Combinations “5 choose 2” = choose 2 items out of 5 choose(5, 2) ## [1] 10 Permutations There is no built-in function to calculate permutations. You can multiply the number of combinations by k!. Ex. Number of permutations of size 2 that can be formed from 5 distinct items: choose(5,2)*factorial(2) ## [1] 20 You can create your own function to do this: perm &lt;- function(n, k) {choose(n,k)*factorial(k)} perm(5,2) ## [1] 20 2.5 Independence Skip everything before “The Multiplication Rule” on p. 86 (until we return to conditional probability later in the semester) "],
["ch-3-discrete-distributions.html", "Ch. 3 Discrete Distributions 3.3 Expected value 3.3 Variance 3.3 Variance (alternative method) 3.4 Binominal Theorem 3.5 Hypergeometric 3.6 Poisson Practice Exercises", " Ch. 3 Discrete Distributions Sections covered: 3.1 - 3.5 3.3 Expected value x &lt;- 1:5 x ## [1] 1 2 3 4 5 px &lt;- c(.1, .15, .2, .25, .3) px ## [1] 0.10 0.15 0.20 0.25 0.30 x*px ## [1] 0.1 0.3 0.6 1.0 1.5 sum(x*px) # E(X) ## [1] 3.5 3.3 Variance x - 3.5 ## [1] -2.5 -1.5 -0.5 0.5 1.5 (x - 3.5)^2 ## [1] 6.25 2.25 0.25 0.25 2.25 ((x - 3.5)^2)*px ## [1] 0.6250 0.3375 0.0500 0.0625 0.6750 sum(((x - 3.5)^2)*px) # V(X) ## [1] 1.75 3.3 Variance (alternative method) x ## [1] 1 2 3 4 5 px ## [1] 0.10 0.15 0.20 0.25 0.30 x^2 ## [1] 1 4 9 16 25 (x^2)*px ## [1] 0.1 0.6 1.8 4.0 7.5 sum((x^2)*px) # E(X^2) ## [1] 14 14-3.5^2 # E(X^2) - [E(X)]^2 ## [1] 1.75 3.4 Binominal Theorem p. 121 Using binomial tables for the cumulative distribution function (cdf) is optional; you may use R (see below) or www.stattrek.com instead. For tests you will not need to calculate these values. You can leave your answers as summations, for example: \\(\\Sigma_{x=0}^6 \\left(\\begin{array}{c}12\\\\ x\\end{array}\\right)(.3^x)(.7^{12-x})\\) R Probability mass function (pmf) \\(P(X = x)\\) choose(8, 3) # &quot;8 choose 3&quot; ## [1] 56 56*.6^3*.4^5 # P(X = 3) given n = 8, p = .6 ## [1] 0.123863 Direct method dbinom(3, 8, .6) # P(X = 3) given n = 8, p = .6 ## [1] 0.123863 Cumulative distribution function (cdf) \\(P(X \\leq x)\\) Find \\(P(2 \\leq X \\leq 4)\\) given \\(p = .7, n = 15\\) pbinom(4, 15, .7) - pbinom(1, 15, .7) ## [1] 0.0006717175 (Using the pmf instead:) dbinom(2, 15, .7) + dbinom(3, 15, .7) + dbinom(4, 15, .7) ## [1] 0.0006717175 Graphing a binomial pmf: ex. \\(p = .7, n = 10\\) px &lt;- dbinom(0:10, 10, .7) # adding las = 1 turns the y-axis tick mark labels horizontal, which are easier to read barplot(px, names.arg = 0:10, las = 1, col = &quot;lightblue&quot;) 3.5 Hypergeometric Note that the notation that R uses is different from the Devore textbook: parameter Devore R total successes M m total failures N-M n sample size n k successes in sample x x Example (p. 127) Devore: h(x; n, M, N) P(X = 2) = h(2; 10, 5, 25) –&gt; dhyper(x = 2, m = 5, n = 20, k = 10) ## [1] 0.3853755 3.6 Poisson Example (p. 132) p(3;2) = dpois(3,2) ## [1] 0.180447 F(3;2) = ppois(3, 2) ## [1] 0.8571235 Practice Exercises (Binomial) Suppose the probability of a car accident involving a single vehicle is .7. If 15 accidents are randomly selected, what is the probability that between 2 and 4, inclusive, involve a single vehicle? (from slides) dbinom(2, 15, .7) + dbinom(3, 15, .7) + dbinom(4, 15, .7) ## [1] 0.0006717175 or pbinom(4, 15, .7) - pbinom(1, 15, .7) ## [1] 0.0006717175 (Binomial) A particular telephone number is used to receive both voice calls and fax messages. Suppose that 25% of the incoming calls involve fax messages, and consider a sample of 25 incoming calls. What is the probability that At most 6 of the calls involve a fax message? Exactly 6 of the calls involve a fax message? At least 6 of the calls involve a fax message? What is the expected number of calls among the 25 that involve a fax message? What is the standard deviation of the number among the 25 calls that involve a fax message? What is the probability that the number of calls among the 25 that involve a fax transmission exceeds the expected number by more than 2 standard deviations? (Textbook 50-51) pbinom(6, 25, .25) ## [1] 0.5610981 dbinom(6, 25, .25) ## [1] 0.1828195 1 - pbinom(5, 25, .25) ## [1] 0.6217215 E &lt;- 25*.25 Var &lt;- 25*.25*.75 pbinom(floor(E+2*sqrt(Var)), 25, .25) ## [1] 0.9703301 (Hypergeometric) A geologist has collected 8 specimens of basaltic rock and 12 specimens of granite. The geologist instructs a laboratory assistant to randomly select 15 of the specimens for analysis. What is the probability that all specimens of one of the two types of rock are selected for analysis? What is the probability that the number of granite specimens selected for analysis is within 1 standard deviation of its mean value? (Textbook 71) dhyper(8, 8, 12, 15) + dhyper(3, 8, 12, 15) ## [1] 0.05469556 mean_g &lt;- 15*12/20 sd_g &lt;- sqrt(20*(12/20)*(8/20)) pbinom(mean_g + sd_g, 15, 12/20)-pbinom(mean_g - sd_g, 15, 12/20) ## [1] 0.8144507 (Negative Binomial) The probability that a randomly selected box of a certain type of cereal has a particular prize is .2. Suppose you purchase box after box until you have obtained two of these prizes. What is the probability that you purchase \\(x\\) boxes that do not have the desired prize? What is the probability that you purchase four boxes? What is the probability that you purchase at most four boxes? How many boxes without the desired prize do you expect to purchase? How many boxes do you expect to purchase? (Textbook 75, homework) Let x be the number of boxes without prizes to be purchased, \\(p = {x+2-1\\choose1}(.2^2)(.8 ^x)\\) dnbinom(2, 2, .2) ## [1] 0.0768 dnbinom(2, 2, .2) + dnbinom(1, 2, .2) + dnbinom(0, 2, .2) ## [1] 0.1808 2*.8/.2 ## [1] 8 # 8 boxes without desired prize, 10 boxes in total (Poisson) Suppose that the number of drivers who travel between a particular origin and destination during a designated time period has a Poisson distribution with parameter \\(\\mu = 20\\). What is the probability that the number of drivers will Be at most 10? Be 10? Be within 2 standard deviations of the mean value? (Textbook 81) ppois(10, 20) # a ## [1] 0.01081172 dpois(10, 20) # b1 ## [1] 0.005816307 ((exp(1)^-20)*(20^10))/factorial(10) # b2 ## [1] 0.005816307 ppois(20 + 2*sqrt(20),20) - ppois(20 - 2*sqrt(20), 20) # c ## [1] 0.9442797 "],
["ch-4-continuous-random-variables-and-probability-distributions.html", "Ch. 4 Continuous Random Variables and Probability Distributions 4.1 Probability density functions R 4.2 Cumulative Distribution Functions and Expected Values 4.3 The Normal Distribution Practice Exercises Extra Practice Chapters 1 - 4", " Ch. 4 Continuous Random Variables and Probability Distributions Sections covered: 4.1, 4.2, 4.3 4.1 Probability density functions R Simulating a uniform distribution Randomly choose one number from the distribution: \\(f(x) = .2\\) for \\(0 \\leq x \\leq 5\\), 0 otherwise. runif(n = 1, min = 0, max = 5) ## [1] 3.309494 Take the average of 1000 picks: x &lt;- runif(n = 1000, min = 0, max = 5) mean(x) ## [1] 2.476738 Draw a histogram of the 1000 picks: hist(x, las = 1, col = &quot;lightblue&quot;) 4.2 Cumulative Distribution Functions and Expected Values Using R to help with calculations in Example 4.7, p. 149 Define a function for the cdf (you have to do the integration yourself): F &lt;- function(x) (x/8) + (3/16)*x^2 Find \\(F(1.5) - F(1)\\) F(1.5) - F(1) ## [1] 0.296875 4.3 The Normal Distribution Interactive normal distribution – change \\(\\mu\\) and \\(\\sigma\\) and see what happens. Normal table – in pdf form for viewing online, downloading and/or printing Online normal distribution lookup – enter \\(z\\) and it will calculate \\(F(z) = P(Z \\leq z)\\). There are many online normal distribution calculators. Note that often you are given the choice of finding \\(P(Z \\leq z)\\), \\(P(Z \\geq z)\\), \\(P(0 \\leq Z \\leq z)\\), etc., so be careful. To be consistent with the normal tables in the textbook / class, choose \\(P(Z \\leq z)\\). Due to rounding differences in \\(z\\) and \\(P(Z \\leq z)\\), solutions using technology will vary from those using a normal table. Both methods are fine. R \\(P(Z \\leq -1)\\) Unless specified otherwise, pnorm uses a mean of 0 and standard deviation of 1 (standard normal). pnorm(-1) ## [1] 0.1586553 \\(P(X \\leq 37)\\) given \\(\\mu = 40\\) and \\(\\sigma = 2\\) pnorm(37, mean = 40, sd = 2) ## [1] 0.0668072 or pnorm((37-40)/2) ## [1] 0.0668072 \\(P(X &gt; 39)\\) 1 - pnorm(39, mean = 40, sd = 2) ## [1] 0.6914625 Find the 75th percentile for the standard normal distribution: qnorm(.75) ## [1] 0.6744898 Find the 75th percentile for a normally distribution population with mean 40 and standard deviation 2: qnorm(.75, mean = 40, sd = 2) ## [1] 41.34898 or 40 + qnorm(.75)*2 ## [1] 41.34898 Practice Exercises (pdf, E and V) \\(X\\) is a continuous random variable with probability distrbution \\(f(x)=.06x + .05\\) where \\(0\\leq x \\leq 5\\). What is the probability that \\(2\\leq x\\leq 4\\)? What is the probability that \\(x = 4\\)? What is the expected value of \\(X\\)? What is the variance of X? [Ans] a. Integrating by hand: \\(f(x)=.06x + .05\\), we get \\(F(x) = .03x^2+.05x\\). F &lt;- function(x) 0.03*x^2 + 0.05*x x &lt;- runif(n = 1000, min = 0, max = 5) F(4) - F(2) ## [1] 0.46 0 Integrate \\(x\\cdot f(x) = x(.06x+.05)=.06x^2+.05x\\), we get \\(H(x) = .02x^3+.025x^2\\). H &lt;- function(x) 0.02*x^3 + 0.025*x^2 H(5)-H(0) ## [1] 3.125 \\(V(X) = E(X^2)-[E(X)]^2\\). Integrate \\(x^2\\cdot f(x) = x^2(.06x+.05)=.06x^3+.05x^2\\), we get \\(G(x) = .015x^4+\\frac{.05}3x^3\\). E &lt;- H(5)-H(0) G &lt;- function(x) .015*(x^4)+(.05/3)*(x^3) G(5)-G(0)-E^2 ## [1] 1.692708 (Standard Normal, pnorm) A driver’s reaction time for an in-traffic response to a brake signal can be modeled with a normal distribution having mean value 1.25 sec and standard deviation of .46 sec. (Textbook 4.16) What is the probability that reaction time is between 1.00 sec and 1.75 sec? What is the probability that reaction time exceeds 2.00 sec? pnorm((1.75-1.25)/0.46) - pnorm((1-1.25)/0.46) ## [1] 0.5680717 1 - pnorm((2-1.25)/0.46) ## [1] 0.05150482 # alternatively pnorm(-(2-1.25)/0.46) ## [1] 0.05150482 (Nonstandard Normal, qnorm) Data collected from an experiment with a specified initial crack length propose a normal distribution with mean value 5.496 mm and standard deviation .067 mm. For this model, what value of final crack depth would be exceeded by only .5% of all cracks? (Textbook 4.18) qnorm(0.995, 5.496, .067) ## [1] 5.668581 (Binom Appproximation) Suppose that 25% of all students at a large public university receive financial aid. Let \\(X\\) be the number of students in a random sample of size 50 who receive financial aid, sothat p = .25. What is the probability that at most 10 students receive aid? (Textbook 4.20) # binom distribution calculation pbinom(10, size = 50, prob = 0.25) ## [1] 0.2622023 # normal approximation of binom pnorm((10 + .5 - 50*.25)/sqrt(50*.25*.75)) ## [1] 0.2568146 Extra Practice Chapters 1 - 4 Questions Solutions "],
["ch-5-random-samples.html", "Ch. 5 Random Samples 5.4 The Distribution of the Sample Mean 5.5 The Distribution of a Linear Combination", " Ch. 5 Random Samples Sections covered: 5.3, 5.4, 5.5 5.4 The Distribution of the Sample Mean Central Limit Theorem visualization: http://mfviz.com/central-limit/ 5.5 The Distribution of a Linear Combination Skip (for now): formula (5.11) – variance of a linear combination not assuming independence of \\(X_i\\)’s. "],
["ch-6-point-estimation.html", "Ch. 6 Point Estimation 6.1 Some General Concepts of Point Estimation", " Ch. 6 Point Estimation Sections covered: 6.1 6.1 Some General Concepts of Point Estimation Skip everything beginning with “Some Complications” on p. 257 "],
["ch-7-statistical-intervals-based-on-a-single-sample.html", "Ch. 7 Statistical Intervals Based on a Single Sample 7.1 Basic Properties of Confidence Intervals 7.2 Large-Sample Confidence Intervals for a Population Mean and Proportion 7.3 Intervals Based on a Normal Population Distribution", " Ch. 7 Statistical Intervals Based on a Single Sample Sections covered: 7.1, 7.2, 7.3 7.1 Basic Properties of Confidence Intervals Skip: “Deriving a Confidence Interval”, pp. 282-284; “Bootstrap Confidence Intervals”, p. 284 7.2 Large-Sample Confidence Intervals for a Population Mean and Proportion You may use: \\(\\hat{p} \\pm z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}\\hat{q}}{n}}\\), rather than the formula (7.10) in the blue box on p. 289 for large sample confidence intervals for proportions, skip p. 290 You may use the method from class \\(n = \\frac{4z^2\\hat{p}\\hat{q}}{w^2}\\) for the minimum \\(n\\) needed to ensure a particular confidence interval width for proportions, rather than formula (7.12) – both appear on p. 291. As noted in Example 7.9 on p. 291, the easier formula gives a slightly different answer (385 instead of 381). Additional textbook material on CI for proportions “When You Hear the Margin of Error Is Plus or Minus 3 Percent, Think 7 Instead”, New York Times, Oct 5, 2016. 7.3 Intervals Based on a Normal Population Distribution Use the \\(t\\) distributions. Skip “A Prediction Interval for a Single Future Value” p. 299 to the end of the section "],
["ch-8-tests-of-hypotheses-based-on-a-single-sample.html", "Ch. 8 Tests of Hypotheses Based on a Single Sample 8.1 Hypotheses and Test Procedures 8.2 z Tests for Hypotheses about a Population Mean 8.3 The One-Sample t Test 8.4 Tests Concerning a Population Proportion Practice Exercises", " Ch. 8 Tests of Hypotheses Based on a Single Sample Sections covered: 8.1, 8.2, 8.3, 8.4 8.1 Hypotheses and Test Procedures Note: We will cover the beginning of the section and then return to errors in hypothesis testing (beginning on page 317) after Section 8.2. Skip: Examples 8.1 and 8.4 (hypothesis testing and type II error calculation for small sample proportions) xkcd cartoon: “Significant” (Jelly Beans Cause Acne!) 8.2 z Tests for Hypotheses about a Population Mean Skip: calculating Type II error and sample size needed for two sided tests (3rd and 5th formulas in the blue box on p. 331) 8.3 The One-Sample t Test Skip: \\(\\beta\\) and Sample Size Determination (p. 338) to end of section 8.4 Tests Concerning a Population Proportion Skip: \\(\\beta\\) and Sample Size Determination (pp. 348-349) Practice Exercises \\(1.\\) (One sample test, Values given) We wish to test whether a scale needs to be recalibrated. If it’s working, a 10 pound weight should weigh 10 pounds. We decide to test it by weighing the same weight, which we know to be precisely 10 pounds, 25 times. We know the weights from this scale are normally distributed and independent, with a true standard deviation of \\(\\sigma\\) = 1. Our sample mean (x) is 9.85. Should we conclude that recalibration is necessary x (that is, the scale is off) or not necessary (the scale isn’t conclusively off)? \\(α = .01 \\) \\(H_0: \\mu = 10\\) \\(H_A: \\mu \\neq 10\\) Test statistic: \\(z=\\frac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{n}}\\) z &lt;- (9.85-10)/(1/sqrt(25)) z ## [1] -0.75 Determine the p-value: pval &lt;- pnorm(z)*2 pval ## [1] 0.4532547 Since 0.453 &gt; .05, we do not reject the null hypothesis. In other words, even if the scale were perfectly calibrated (\\(\\mu = 10\\)) there is over a 45% probability that we would get a sample mean less than or equal to 9.85 (or equal to or greater than 10.15), that is, the same or further from the true mean than we found. Therefore, on the basis on these results, recaliberation is not necessary. \\(2.\\) (One sample test, Raw values) We know that the mean sepal length of setosas (a species of irises) is 4.8 cm. A new study examines a sample of 50 flowers and shows that the sample mean is 5.006 cm. Conduct a one-tailed hypothesis test at \\(.05\\) significance level to show if there is sufficient evidence to reject the hypothesis that the mean of the new sample is equal to 4.8 cm. \\(α = .05\\) \\(H_0: \\mu = 4.8\\) \\(H_A: \\mu &gt; 4.8\\) Use the following code to store the 50 values in the sample in a variable called setosa. (This works because iris is a built-in dataset in R.) setosa &lt;- iris[iris$Species == &quot;setosa&quot;,] [Ans] Test statistic: \\(z=\\frac{\\bar{X} - \\mu_0}{s/\\sqrt{n}}\\) z &lt;- (mean(setosa$Sepal.Length)-4.8)/sqrt(var(setosa$Sepal.Length)/50) z ## [1] 4.132433 pval &lt;- 1- pnorm(z) pval ## [1] 0.00001794718 # simpler approach # p-values might be a bit different due to approximation of t t.test(setosa$Sepal.Length, alternative = &quot;greater&quot;, mu = 4.8) ## ## One Sample t-test ## ## data: setosa$Sepal.Length ## t = 4.1324, df = 49, p-value = 0.00006986 ## alternative hypothesis: true mean is greater than 4.8 ## 95 percent confidence interval: ## 4.922425 Inf ## sample estimates: ## mean of x ## 5.006 Since 6.986e-05 &lt; 0.05 , we reject the null hypothesis. There is sufficient evidence that the true mean of setosa sepal length is greater than 4.8 cm. "],
["ch-9-inferences-based-on-two-samples.html", "Ch. 9 Inferences Based on Two Samples 9.1 \\(z\\) Tests and CI’s for a Difference Between Two Population Means 9.2 The Two-Sample \\(t\\) Test and CI 9.3 Analysis of Paired Data 9.4 Inferences Concerning a Difference Between Population Proportions", " Ch. 9 Inferences Based on Two Samples Sections covered: 9.1, 9.2, 9.3, 9.4 9.1 \\(z\\) Tests and CI’s for a Difference Between Two Population Means (Cases 1 &amp; 2) Skip: \\(\\beta\\) and the Choice of Sample Size (pp. 366-367) R Since you aren’t given the original data, R isn’t very helpful here, but you can write a function that you could reuse, such as: options(scipen = 999) # get rid of scientific notation # this function only has to be run once per session, and then you can reuse it. diffmeans &lt;- function(xbar, ybar, delta0, sigma1, sigma2, m, n, type = &quot;twosided&quot;) { Z &lt;- ((xbar - ybar) - delta0)/sqrt(((sigma1^2)/m) + ((sigma2^2)/n)) if (type == &quot;twosided&quot;) { pvalue &lt;- pnorm(-abs(Z))*2 } else if (type == &quot;lowertail&quot;) { pvalue &lt;- pnorm(Z) } else { pvalue &lt;- 1 - pnorm(Z) } print(c(&quot;The p-value is&quot;, round(pvalue, 4))) } # Example 9.1, p. 365 diffmeans(xbar = 29.8, ybar = 34.7, delta0 = 0, sigma1 = 4, sigma2 = 5, m = 20, n = 25, type = &quot;twosided&quot;) ## [1] &quot;The p-value is&quot; &quot;0.0003&quot; # Example 9.4, p. 368 # As long as you use the correct order of parameters, you don&#39;t have to write out the names: diffmeans(2258, 2637, -200, 1519, 1138, 663, 413, &quot;lowertail&quot;) ## [1] &quot;The p-value is&quot; &quot;0.0139&quot; 9.2 The Two-Sample \\(t\\) Test and CI (Case 3) Use: http://bit.ly/degreesoffreedom to calculate the degrees of freedom Skip: Pooled \\(t\\) Procedures (pp. 377-378) Skip: Type II Error Probabilities (pp. 378-379) Given two random samples, use t.test() with different parameters to carry out a two-sample hypothesis test. For demonstration purposes, x and y here are samples of 10 numbers that drawn from the standard normal distribution. set.seed(1) x &lt;- rnorm(10) set.seed(2) y &lt;- rnorm(10) t.test(x, y, var.equal = TRUE, alternative = &quot;less&quot;) ## ## Two Sample t-test ## ## data: x and y ## t = -0.19865, df = 18, p-value = 0.4224 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf 0.610222 ## sample estimates: ## mean of x mean of y ## 0.1322028 0.2111516 Thus, H_0 is rejected at alpha = .05. In t.test(), “less” indicates that H_A: delta &lt; 0. Also try using alternative = \"two.sided\" or alternative = \"two.sided\" for a different alternative hypothesis. 9.3 Analysis of Paired Data Skip: Paired Data and Two-Sample \\(t\\) Procedures (pp. 386-387) Skip: Paired Versus Unpaired Experiments (pp. 387-388) Two ways of doing paired test: (Here we continue to use the x and y in section 9.2 above) [Method 1] Take x-y and do a one-sample test. t.test(x-y, alternative = &quot;less&quot;) ## ## One Sample t-test ## ## data: x - y ## t = -0.17952, df = 9, p-value = 0.4308 ## alternative hypothesis: true mean is less than 0 ## 95 percent confidence interval: ## -Inf 0.7272152 ## sample estimates: ## mean of x ## -0.07894886 [Method 2] Give another parameter paired = TRUE. In R, the default parameter for paired in t.test() is FALSE; here we set it to TRUE and leave x and y as two separate inputs. t.test(x, y, alternative = &quot;less&quot;, paired = TRUE) ## ## Paired t-test ## ## data: x and y ## t = -0.17952, df = 9, p-value = 0.4308 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf 0.7272152 ## sample estimates: ## mean of the differences ## -0.07894886 It’s clear that they give the same results. 9.4 Inferences Concerning a Difference Between Population Proportions Skip: Type II Error Probabilities and Sample Sizes (pp. 394-395) R A/B Testing question from class: clicks &lt;- c(25, 20) people &lt;- c(100, 100) prop.test(clicks, people, correct = FALSE) ## ## 2-sample test for equality of proportions without continuity ## correction ## ## data: clicks out of people ## X-squared = 0.71685, df = 1, p-value = 0.3972 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## -0.06553817 0.16553817 ## sample estimates: ## prop 1 prop 2 ## 0.25 0.20 "],
["ch-2-conditional-probability.html", "Ch. 2 Conditional Probability Resources", " Ch. 2 Conditional Probability Sections covered: 2.4, 2.5 (Section 2.5 was covered earlier; now we’re adding the definition of independence in terms of conditional probability on p. 85: “Two events A and B are independent if P(A|B) = P(A) and are dependent otherwise.”) Resources Extra practice problems, Bayes Theorem (Posted on Twitter as a response to Bill Gates’ provocative comment that he’d rather encounter a shark than a mosquito in the wild.) An Intuitive (and Short) Explanation of Bayes’ Theorem "],
["ch-5-joint-probability.html", "Ch. 5 Joint Probability 5.1 Jointly Distributed Random Variables 5.2 Expected Values, Covariance, and Correlation Resources", " Ch. 5 Joint Probability Sections covered: 5.1, 5.2 5.1 Jointly Distributed Random Variables Skip everything but “Two Discrete Random Variables” pp. 199-200 5.2 Expected Values, Covariance, and Correlation Skip everything but “Covariance” pp. 214-215 and “Correlation” pp. 216-218 (In both sections, skip double integrals) Resources Correlation and Covariance Visualization https://shiny.rit.albany.edu/stat/rectangles/ "],
["ch-14-chi-squared-test.html", "Ch. 14 Chi Squared Test 14.3 Two-Way Contingency Tables Resources R Practice Exercises", " Ch. 14 Chi Squared Test Sections covered: 14.3 14.3 Two-Way Contingency Tables Skip: pp. 639-643, including “Testing for Homogeneity” Focus on “Testing for Independence (Lack of Association)” Notes on the chi square test formula on p. 644: Write the null hypothesis as a sentence, not as in the book. (For example: “Class and Survival Status are independent.”) “estimated expected” in the textbook is the same as “expected” used in class I and J refer to the number of rows and columns in the table Resources Chi Square Table Chi Squared Test Calculator Chi Squared Distribution Curves R The chisq.test() function requires that data be in matrix form: # p. 647, #28 mat &lt;- matrix(c(28, 17, 7, 31, 26, 10, 26, 19, 11), nrow = 3, byrow = TRUE) dimnames(mat) &lt;- list(`Email_Provider` = c(&quot;gmail&quot;, &quot;Yahoo&quot;, &quot;Other&quot;), `Cell_Phone_Provider` = c(&quot;ATT&quot;, &quot;Verizon&quot;, &quot;Other&quot;)) chisq.test(mat, correct = FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: mat ## X-squared = 1.5074, df = 4, p-value = 0.8253 To see the expected values: results &lt;- chisq.test(mat, correct = FALSE) round(results$expected, 2) ## Cell_Phone_Provider ## Email_Provider ATT Verizon Other ## gmail 25.26 18.42 8.32 ## Yahoo 32.54 23.74 10.72 ## Other 27.20 19.84 8.96 Mosaic plot mosaicplot(t(mat), color = c(&quot;aliceblue&quot;, &quot;cornflowerblue&quot;, &quot;navyblue&quot;), main = &quot;&quot;) See this tutorial for more on mosaic plots. Practice Exercises (Class example) We took a survey involving 20 children and 80 adults. 1 of the children and 49 of the adults drink coffee, while the remainder do not. Does there appear to be a relationship between age (child vs. adult) and coffee drinking status (yes vs. no)? [Ans] x &lt;- c(1, 49, 19, 31) dim(x) &lt;- c(2, 2) x ## [,1] [,2] ## [1,] 1 19 ## [2,] 49 31 chisq.test(x, correct = FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: x ## X-squared = 20.25, df = 1, p-value = 0.000006795 (Hypothesis Testing) In an investigation of alcohol use among college students, each male student in a sample was categorized both according to age group and according to the number of heavy drinking episodes during the previous 30 days. 18-23 21-23 \\(\\geq\\) 24 None 357 293 592 1-2 218 285 354 3-4 184 218 185 \\(\\geq\\) 5 328 331 147 Does there appear to be an association between extent of binge drinking and age group in the population from which the sample was selected? Carry out a test of hypotheses at significance level .01. (Testbook 14.25) [Ans] \\(α = .01\\) \\(H_0\\): the extent of binge drinking and age group are independent \\(H_A\\): the extent of binge drinking and age group are not independent. data &lt;- c(357, 218, 184, 328, 293, 285, 218, 331, 592, 354, 185, 147) dim(data) &lt;- c(4, 3) data ## [,1] [,2] [,3] ## [1,] 357 293 592 ## [2,] 218 285 354 ## [3,] 184 218 185 ## [4,] 328 331 147 # optional: give names to the rows and columns dimnames(data) &lt;- list(`Episodes` = c(&quot;None&quot;, &quot;1-2&quot;, &quot;3-4&quot;, &quot;&gt;= 5&quot;),`Age Group` = c(&quot;18-23&quot;, &quot;21-23&quot;, &quot;&gt;=24&quot;)) data ## Age Group ## Episodes 18-23 21-23 &gt;=24 ## None 357 293 592 ## 1-2 218 285 354 ## 3-4 184 218 185 ## &gt;= 5 328 331 147 chisq.test(data, correct = FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: data ## X-squared = 212.91, df = 6, p-value &lt; 0.00000000000000022 Alternatively, we can use R as a calculator. total &lt;- sum(data) prob &lt;- data/total expected &lt;- data # initialize matrix &quot;expected&quot; to be the same size as &quot;data&quot; for (i in 1:4){ for(j in 1:3){ expected[i,j] &lt;- sum(prob[i,])*sum(prob[,j])*total } } chisq &lt;- sum((data-expected)^2/expected) chisq ## [1] 212.9072 \\(p-value &lt; .01\\) Reject \\(H_0\\). There is an association between extent of binge drinking and age group in the population from which the sample was selected. "],
["ch-12-linear-regression.html", "Ch. 12 Linear Regression 12.1 The Simple Linear Regression Model 12.2 Estimating Model Parameters 12.5 Correlation Practice Exercises", " Ch. 12 Linear Regression Sections covered: 12.1, 12.2, 12.5 12.1 The Simple Linear Regression Model 12.2 Estimating Model Parameters Resources Interactive Visualization: Linear Regression Try fitting the least squares line to a set of random data and check your answer (and another one). Video: Regression I: What is regression? | SSE, SSR, SST | R-squared | Errors (ε vs. e) [contributed by Lance J.] R Calculating slope and intercept for a sample of (x, y) pairs (p. 498 formulas) # Example 12.8, p. 503 x &lt;- c(12, 30, 36, 40, 45, 57, 62, 67, 71, 78, 93, 94, 100, 105) y &lt;- c(3.3, 3.2, 3.4, 3, 2.8, 2.9, 2.7, 2.6, 2.5, 2.6, 2.2, 2, 2.3, 2.1) lm(y ~ x) #lm = linear model ## ## Call: ## lm(formula = y ~ x) ## ## Coefficients: ## (Intercept) x ## 3.62091 -0.01471 Predicted values: mod &lt;- lm(y~x) round(mod$fitted.values, 2) ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## 3.44 3.18 3.09 3.03 2.96 2.78 2.71 2.64 2.58 2.47 2.25 2.24 2.15 2.08 Residuals: round(mod$residuals, 2) ## 1 2 3 4 5 6 7 8 9 10 11 12 ## -0.14 0.02 0.31 -0.03 -0.16 0.12 -0.01 -0.04 -0.08 0.13 -0.05 -0.24 ## 13 14 ## 0.15 0.02 SSE, SSR anova(mod) # anova = analysis of variance ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 1 2.29469 2.29469 104.92 0.0000002762 *** ## Residuals 12 0.26246 0.02187 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The first row under “Sum Sq” is the SSR, and the second row under “Sum Sq” is the SSE: SSE = 0.2624565 SSR = 2.2946864 SST = SSE + SSR = 0.2624565 + 2.2946864 = 2.5571 coefficient of determination \\(r^2\\) # Example 12.4, 12.9 x &lt;- c(132, 129, 120, 113.2, 105, 92, 84, 83.2, 88.4, 59, 80, 81.5, 71, 69.2) y &lt;- c(46, 48, 51, 52.1, 54, 52, 59, 58.7, 61.6, 64, 61.4, 54.6, 58.8, 58) mod &lt;- lm(y ~ x) SSE &lt;- anova(mod)$`Sum Sq`[1] SST &lt;- anova(mod)$`Sum Sq`[1] + anova(mod)$`Sum Sq`[2] 1 - (SSE/SST) ## [1] 0.2092398 Or (simply): cor(x,y)^2 ## [1] 0.7907602 (See section 12.5) 12.5 Correlation Skip: “Inferences About the Population Correlation Coefficient” (p. 530) to end of section. Resources Interactive visualization: Correlation Coefficient (add and remove points) Interactive visualization: Interpreting Correlations [contributed by Dario G.] R sample correlation coefficient \\(r\\) # Example 12.15, p. 528 x &lt;- c(2.4, 3.4, 4.6, 3.7, 2.2, 3.3, 4.0, 2.1) y &lt;- c(1.33, 2.12, 1.80, 1.65, 2.00, 1.76, 2.11, 1.63) cor(x,y) ## [1] 0.3472602 Practice Exercises (Least squares line)** Researchers employed a least squares analysis in studying how \\(Y=\\) porosity (%) is related to \\(X=\\) unit weight (pcf) in concrete specimens. Consider the following representative data: x &lt;- c(99.0, 101.1, 102.7, 103.0, 105.4, 107.0, 108.7, 110.8, 112.1, 112.4, 113.6, 113.8, 115.1, 115.4, 120.0) y &lt;- c(28.8, 27.9, 27.0, 25.2, 22.8, 21.5, 20.9, 19.6, 17.1, 18.9, 16.0, 16.7, 13.0, 13.6, 10.8) (Textbook 12.17) Obtain the equation of the estimated regression line. lm(y~x) ## ## Call: ## lm(formula = y ~ x) ## ## Coefficients: ## (Intercept) x ## 118.9099 -0.9047 \\(y = 118.91 - 0.9047x\\) Calculate the residuals corresponding to the first two observations. mod &lt;- lm(y~x) round(mod$residuals, 2) ## 1 2 3 4 5 6 7 8 9 10 11 12 ## -0.54 0.46 1.01 -0.52 -0.75 -0.60 0.33 0.93 -0.39 1.68 -0.13 0.75 ## 13 14 15 ## -1.78 -0.90 0.46 Or alternatively, use R as a calculator pred &lt;- 118.9099 - 0.9047*x res &lt;- y - pred res[1] ## [1] -0.5446 res[2] ## [1] 0.45527 Calculate a point estimate of \\(\\sigma\\). sig2 &lt;- sum((res)^2)/(length(x)-2) sqrt(sig2) ## [1] 0.938042 What proportion of observed variation in porosity can be attributed to the approximate linear relationship between unit weight and porosity? cor(x, y)^2 ## [1] 0.9738874 Calculate the SSE and SST. anova(mod) # analsis of variance ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 1 426.62 426.62 484.84 0.00000000001125 *** ## Residuals 13 11.44 0.88 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 SSE &lt;- anova(mod)$`Sum Sq`[1] SST &lt;- anova(mod)$`Sum Sq`[1] + anova(mod)$`Sum Sq`[2] c(SSE, SST) ## [1] 426.6185 438.0573 Or alternatively, use R as a calculator. Notice that the same results are produced. SSE1 &lt;- sum((mod$residual)^2) SST1 &lt;- sum((y-mean(y))^2) SSR1 &lt;- sum((mod$fitted.values - mean(y))^2) c(SSE1, SST1, SSE1+SSR1) ## [1] 11.43883 438.05733 438.05733 "],
["ch-13-nonlinear-and-multiple-regression.html", "Ch. 13 Nonlinear and Multiple Regression 13.1 Assessing Model Adequacy", " Ch. 13 Nonlinear and Multiple Regression Sections covered: 13.1 13.1 Assessing Model Adequacy Skip: standardized residuals Residuals plots: know \\(e\\) vs. \\(x\\) only Difficulties and remedies: 1. - 4. – know the difficulties, not the remedies Skip: weighted least squares (p. 547) "]
]
