[
["index.html", "Stat 1201 Supplemental Materials Introduction General study tips Installing R Getting Started with R Contact License", " Stat 1201 Supplemental Materials Joyce Robbins 2019-07-18 Introduction This site contains supplemental materials for Stat 1201, mainly: 1) clarifications on which sections we cover in the textbook (Devore, Probability and Statistics for Engineering and the Sciences 9th edition), 2) R code, and 3) links to helpful resources online. It is not in any way a substitute for materials available in CourseWorks. If you find additional online resources that are helpful to this class, please create an issue or send me an email and I’ll add them to this resource. Let me know as well if you find any typos or other mistakes. Note that while you’re encouraged to look ahead, be sure to circle back to those sections when they’re covered in class since content may be added or modified slightly. General study tips The website for the book Make It Stick offers a summary of the experimentally tested study strategies. The tl;dr is: working out problems is better than reviewing notes / textbook doing mixed reviews is better than focusing on one type of problem at a time learning is hard work; if it seems too easy your study strategy might not be the most effective making mistakes and learning from them is a useful strategy (don’t wait until you’ve mastered all of the examples to try a problem) You’ve likely heard a lot of these ideas before, but it’s worth really thinking about them and putting them into practice. My advice: As you’re reading the textbook or working on a problem set, keep a list of questions. Challenge yourself by thinking about how the problem would differ if you changed the setup. Try creating your own questions and solving them. Try solving problems in multiple ways. Learn from a variety of sources: class, textbook, Cartoon Guide, etc. If you find differences, ask. Installing R You will need to install two applications: R and RStudio: R – the programming language itself – is available here: https://cran.r-project.org RStudio – an integrated development environment (IDE) which makes it much easier to use R. It is optional but highly recommended. This is the app you will open to use R. https://www.rstudio.com/products/rstudio/download/#download Getting Started with R To get oriented, read and try the examples in Chapter 1 of Introduction to R Quick intro to RStudio Working in the console pane Assigning a variable Drawing a stem and leaf plot Saving your work Converting your work to an .html file that can be submitted to CourseWorks Using R as a calculator Basic operations: 3 + 4 ## [1] 7 3 - 4 ## [1] -1 3 * 4 ## [1] 12 3 / 4 ## [1] 0.75 3^4 ## [1] 81 Working with vectors: x &lt;- 1:5 x ## [1] 1 2 3 4 5 sum(x) ## [1] 15 cumsum(x) ## [1] 1 3 6 10 15 px &lt;- x*.05 + .05 px ## [1] 0.10 0.15 0.20 0.25 0.30 Expected value: x*px ## [1] 0.1 0.3 0.6 1.0 1.5 sum(x*px) ## [1] 3.5 Contact Joyce Robbins: Columbia Profile GitHub License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. "],
["ch-1-descriptive-statistics.html", "Ch. 1 Descriptive Statistics 1.1 Populations, Samples, and Processes 1.2 Pictorial and Tabular Methods in Descriptive Statistics 1.3 Measures of location 1.4 Measures of variability", " Ch. 1 Descriptive Statistics Sections covered: all 1.1 Populations, Samples, and Processes 1.2 Pictorial and Tabular Methods in Descriptive Statistics Skip: “Dotplots” pp. 15-16 R Stem-and-leaf display: prices &lt;- c(379, 425, 450, 450, 499, 529, 535, 535, 545, 599, 665, 675, 699, 699, 725, 725, 745, 799) stem(prices) ## ## The decimal point is 2 digit(s) to the right of the | ## ## 3 | 8 ## 4 | 355 ## 5 | 03445 ## 6 | 078 ## 7 | 00335 ## 8 | 0 Note that histograms are drawn with unbinned data. R does the binning in the process of drawing the histogram. Frequency histogram: prices &lt;- c(379, 425, 450, 450, 499, 529, 535, 535, 545, 599, 665, 675, 699, 699, 725, 725, 745, 799) hist(prices) hist(prices, breaks = c(300, 400, 500, 600, 700, 800), col = &quot;lightblue&quot;) Density histogram: prices &lt;- c(379, 425, 450, 450, 499, 529, 535, 535, 545, 599, 665, 675, 699, 699, 725, 725, 745, 799) hist(prices, freq = FALSE, breaks = c(300, 400, 500, 600, 700, 800), col = &quot;lightblue&quot;, las = 1) 1.3 Measures of location R prices &lt;- c(379, 425, 450, 450, 499, 529, 535, 535, 545, 599, 665, 675, 699, 699, 725, 725, 745, 799) mean(prices) ## [1] 593.2222 median(prices) ## [1] 572 ## quartiles quantile(prices) ## 0% 25% 50% 75% 100% ## 379.0 506.5 572.0 699.0 799.0 ## trimmed mean mean(prices, trim = .1) ## 10% trimmed mean ## [1] 593.75 1.4 Measures of variability Skip: extreme outliers (p. 42) We will define outliers for boxplots to be observations that are more than 1.5 times the fourth spread from the closest fourth. They may be indicated with either a solid or open circle (in contrast to the book which uses one for mild outliers and the other for extreme outliers.) R Sample variance: prices &lt;- c(379, 425, 450, 450, 499, 529, 535, 535, 545, 599, 665, 675, 699, 699, 725, 725, 745, 799) var(prices) ## [1] 15981.48 Sample standard deviation: sqrt(var(prices)) ## [1] 126.4179 sd(prices) ## [1] 126.4179 Five number summary (min, lower-hinge, median, upper-hinge, max) fivenum(prices) ## [1] 379 499 572 699 799 Boxplots prices &lt;- c(379, 425, 450, 450, 499, 529, 535, 535, 545, 599, 665, 675, 699, 699, 725, 725, 745, 799) boxplot(prices) boxplot(prices, horizontal = TRUE) PTSD &lt;- c(10, 20, 25, 28, 31, 35, 37, 38, 38, 39, 39, 42, 46) Healthy &lt;- c(23, 39, 40, 41, 43, 47, 51, 58, 63, 66, 67, 69, 72) df &lt;- data.frame(Healthy, PTSD) boxplot(df, horizontal = TRUE) "],
["ch-2-probability.html", "Ch. 2 Probability 2.1 Sample Spaces and Events 2.2 Axioms, Interpretations, and Properties of Probability 2.3 Counting Techniques 2.5 Independence", " Ch. 2 Probability Sections covered: 2.1, 2.2, 2.3, 2.5 (2.4 will be covered later in the semester) 2.1 Sample Spaces and Events Humor Source: https://twitter.com/bcrypt/status/1074415553266122752/photo/1 2.2 Axioms, Interpretations, and Properties of Probability 2.3 Counting Techniques R Factorial factorial(5) ## [1] 120 Combinations “5 choose 2” = choose 2 items out of 5 choose(5, 2) ## [1] 10 Permutations There is no built-in function to calculate permutations. You can multiply the number of combinations by k!. Ex. Number of permutations of size 2 that can be formed from 5 distinct items: choose(5,2)*factorial(2) ## [1] 20 You can create your own function to do this: perm &lt;- function(n, k) {choose(n,k)*factorial(k)} perm(5,2) ## [1] 20 2.5 Independence Skip everything before “The Multiplication Rule” on p. 86 (until we return to conditional probability later in the semester) "],
["ch-3-discrete-distributions.html", "Ch. 3 Discrete Distributions 3.3 Expected value 3.3 Variance 3.3 Variance (alternative method) 3.4 Binominal Theorem 3.5 Hypergeometric 3.6 Poisson", " Ch. 3 Discrete Distributions Sections covered: 3.1 - 3.5 3.3 Expected value x &lt;- 1:5 x ## [1] 1 2 3 4 5 px &lt;- c(.1, .15, .2, .25, .3) px ## [1] 0.10 0.15 0.20 0.25 0.30 x*px ## [1] 0.1 0.3 0.6 1.0 1.5 sum(x*px) # E(X) ## [1] 3.5 3.3 Variance x - 3.5 ## [1] -2.5 -1.5 -0.5 0.5 1.5 (x - 3.5)^2 ## [1] 6.25 2.25 0.25 0.25 2.25 ((x - 3.5)^2)*px ## [1] 0.6250 0.3375 0.0500 0.0625 0.6750 sum(((x - 3.5)^2)*px) # V(X) ## [1] 1.75 3.3 Variance (alternative method) x ## [1] 1 2 3 4 5 px ## [1] 0.10 0.15 0.20 0.25 0.30 x^2 ## [1] 1 4 9 16 25 (x^2)*px ## [1] 0.1 0.6 1.8 4.0 7.5 sum((x^2)*px) # E(X^2) ## [1] 14 14-3.5^2 # E(X^2) - [E(X)]^2 ## [1] 1.75 3.4 Binominal Theorem p. 121 Using binomial tables for the cumulative distribution function (cdf) is optional; you may use R (see below) or www.stattrek.com instead. For tests you will not need to calculate these values. You can leave your answers as summations, for example: \\(\\Sigma_{x=0}^6 \\left(\\begin{array}{c}12\\\\ x\\end{array}\\right)(.3^x)(.7^{12-x})\\) R Probability mass function (pmf) \\(P(X = x)\\) choose(8, 3) # &quot;8 choose 3&quot; ## [1] 56 56*.6^3*.4^5 # P(X = 3) given n = 8, p = .6 ## [1] 0.123863 Direct method dbinom(3, 8, .6) # P(X = 3) given n = 8, p = .6 ## [1] 0.123863 Cumulative distribution function (cdf) \\(P(X \\leq x)\\) Find \\(P(2 \\leq X \\leq 4)\\) given \\(p = .7, n = 15\\) pbinom(4, 15, .7) - pbinom(1, 15, .7) ## [1] 0.0006717175 (Using the pmf instead:) dbinom(2, 15, .7) + dbinom(3, 15, .7) + dbinom(4, 15, .7) ## [1] 0.0006717175 Graphing a binomial pmf: ex. \\(p = .7, n = 10\\) px &lt;- dbinom(0:10, 10, .7) # adding las = 1 turns the y-axis tick mark labels horizontal, which are easier to read barplot(px, names.arg = 0:10, las = 1, col = &quot;lightblue&quot;) 3.5 Hypergeometric Note that the notation that R uses is different from the Devore textbook: parameter Devore R total successes M m total failures N-M n sample size n k successes in sample x x Example (p. 127) Devore: h(x; n, M, N) P(X = 2) = h(2; 10, 5, 25) –&gt; dhyper(x = 2, m = 5, n = 20, k = 10) ## [1] 0.3853755 3.6 Poisson Example (p. 132) p(3;2) = dpois(3,2) ## [1] 0.180447 F(3;2) = ppois(3, 2) ## [1] 0.8571235 "],
["ch-4-continuous-random-variables-and-probability-distributions.html", "Ch. 4 Continuous Random Variables and Probability Distributions 4.1 Probability density functions R 4.2 Cumulative Distribution Functions and Expected Values 4.3 The Normal Distribution Extra Practice Chapters 1 - 4", " Ch. 4 Continuous Random Variables and Probability Distributions Sections covered: 4.1, 4.2, 4.3 4.1 Probability density functions R Simulating a uniform distribution Randomly choose one number from the distribution: \\(f(x) = .2\\) for \\(0 \\leq x \\leq 5\\), 0 otherwise. runif(n = 1, min = 0, max = 5) ## [1] 3.070118 Take the average of 1000 picks: x &lt;- runif(n = 1000, min = 0, max = 5) mean(x) ## [1] 2.469509 Draw a histogram of the 1000 picks: hist(x, las = 1, col = &quot;lightblue&quot;) 4.2 Cumulative Distribution Functions and Expected Values Using R to help with calculations in Example 4.7, p. 149 Define a function for the cdf (you have to do the integration yourself): F &lt;- function(x) (x/8) + (3/16)*x^2 Find \\(F(1.5) - F(1)\\) F(1.5) - F(1) ## [1] 0.296875 4.3 The Normal Distribution Interactive normal distribution – change \\(\\mu\\) and \\(\\sigma\\) and see what happens. Normal table – in pdf form for viewing online, downloading and/or printing Online normal distribution lookup – enter \\(z\\) and it will calculate \\(F(z) = P(Z \\leq z)\\). There are many online normal distribution calculators. Note that often you are given the choice of finding \\(P(Z \\leq z)\\), \\(P(Z \\geq z)\\), \\(P(0 \\leq Z \\leq z)\\), etc., so be careful. To be consistent with the normal tables in the textbook / class, choose \\(P(Z \\leq z)\\). Due to rounding differences in \\(z\\) and \\(P(Z \\leq z)\\), solutions using technology will vary from those using a normal table. Both methods are fine. R \\(P(Z \\leq -1)\\) Unless specified otherwise, pnorm uses a mean of 0 and standard deviation of 1 (standard normal). pnorm(-1) ## [1] 0.1586553 \\(P(X \\leq 37)\\) given \\(\\mu = 40\\) and \\(\\sigma = 2\\) pnorm(37, mean = 40, sd = 2) ## [1] 0.0668072 or pnorm((37-40)/2) ## [1] 0.0668072 \\(P(X &gt; 39)\\) 1 - pnorm(39, mean = 40, sd = 2) ## [1] 0.6914625 Find the 75th percentile for the standard normal distribution: qnorm(.75) ## [1] 0.6744898 Find the 75th percentile for a normally distribution population with mean 40 and standard deviation 2: qnorm(.75, mean = 40, sd = 2) ## [1] 41.34898 or 40 + qnorm(.75)*2 ## [1] 41.34898 Extra Practice Chapters 1 - 4 Questions Solutions "],
["ch-5-random-samples.html", "Ch. 5 Random Samples 5.4 The Distribution of the Sample Mean 5.5 The Distribution of a Linear Combination", " Ch. 5 Random Samples Sections covered: 5.3, 5.4, 5.5 5.4 The Distribution of the Sample Mean Central Limit Theorem visualization: http://mfviz.com/central-limit/ 5.5 The Distribution of a Linear Combination Skip (for now): formula (5.11) – variance of a linear combination not assuming independence of \\(X_i\\)’s. "],
["ch-6-point-estimation.html", "Ch. 6 Point Estimation 6.1 Some General Concepts of Point Estimation", " Ch. 6 Point Estimation Sections covered: 6.1 6.1 Some General Concepts of Point Estimation Skip everything beginning with “Some Complications” on p. 257 "],
["ch-7-statistical-intervals-based-on-a-single-sample.html", "Ch. 7 Statistical Intervals Based on a Single Sample 7.1 Basic Properties of Confidence Intervals 7.2 Large-Sample Confidence Intervals for a Population Mean and Proportion 7.3 Intervals Based on a Normal Population Distribution", " Ch. 7 Statistical Intervals Based on a Single Sample Sections covered: 7.1, 7.2, 7.3 7.1 Basic Properties of Confidence Intervals Skip: “Deriving a Confidence Interval”, pp. 282-284; “Bootstrap Confidence Intervals”, p. 284 7.2 Large-Sample Confidence Intervals for a Population Mean and Proportion You may use: \\(\\hat{p} \\pm z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}\\hat{q}}{n}}\\), rather than the formula (7.10) in the blue box on p. 289 for large sample confidence intervals for proportions, skip p. 290 You may use the method from class \\(n = \\frac{4z^2\\hat{p}\\hat{q}}{w^2}\\) for the minimum \\(n\\) needed to ensure a particular confidence interval width for proportions, rather than formula (7.12) – both appear on p. 291. As noted in Example 7.9 on p. 291, the easier formula gives a slightly different answer (385 instead of 381). Additional textbook material on CI for proportions “When You Hear the Margin of Error Is Plus or Minus 3 Percent, Think 7 Instead”, New York Times, Oct 5, 2016. 7.3 Intervals Based on a Normal Population Distribution Use the \\(t\\) distributions. Skip “A Prediction Interval for a Single Future Value” p. 299 to the end of the section "],
["ch-8-tests-of-hypotheses-based-on-a-single-sample.html", "Ch. 8 Tests of Hypotheses Based on a Single Sample 8.1 Hypotheses and Test Procedures 8.2 z Tests for Hypotheses about a Population Mean 8.3 The One-Sample t Test 8.4 Tests Concerning a Population Proportion", " Ch. 8 Tests of Hypotheses Based on a Single Sample Sections covered: 8.1, 8.2, 8.3, 8.4 8.1 Hypotheses and Test Procedures Note: We will cover the beginning of the section and then return to errors in hypothesis testing (beginning on page 317) after Section 8.2. Skip: Examples 8.1 and 8.4 (hypothesis testing and type II error calculation for small sample proportions) xkcd cartoon: “Significant” (Jelly Beans Cause Acne!) 8.2 z Tests for Hypotheses about a Population Mean Skip: calculating Type II error and sample size needed for two sided tests (3rd and 5th formulas in the blue box on p. 331) Example: We wish to test whether a scale needs to be recalibrated. If it’s working, a 10 pound weight should weigh 10 pounds. We decide to test it by weighing the same weight, which we know to be precisely 10 pounds, 25 times. We know the weights from this scale are normally distributed and independent, with a true standard deviation of \\(\\sigma\\) = 1. Our sample mean (x) is 9.85. Should we conclude that recalibration is necessary x (that is, the scale is off) or not necessary (the scale isn’t conclusively off)? \\(α = .01 \\) \\(H_0: \\mu = 10\\) \\(H_A: \\mu \\neq 10\\) Test statistic: \\(z=\\frac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{n}}\\) z &lt;- (9.85-10)/(1/sqrt(25)) z ## [1] -0.75 Determine the p-value: pval &lt;- pnorm(z)*2 pval ## [1] 0.4532547 Since 0.453 &gt; .05, we do not reject the null hypothesis. In other words, even if the scale were perfectly calibrated (\\(\\mu = 10\\)) there is over a 45% probability that we would get a sample mean less than or equal to 9.85 (or equal to or greater than 10.15), that is, the same or further from the true mean than we found. Therefore, on the basis on these results, recaliberation is not necessary. 8.3 The One-Sample t Test Skip: \\(\\beta\\) and Sample Size Determination (p. 338) to end of section 8.4 Tests Concerning a Population Proportion Skip: \\(\\beta\\) and Sample Size Determination (pp. 348-349) "],
["ch-9-inferences-based-on-two-samples.html", "Ch. 9 Inferences Based on Two Samples 9.1 \\(z\\) Tests and CI’s for a Difference Between Two Population Means 9.2 The Two-Sample \\(t\\) Test and CI 9.3 Analysis of Paired Data 9.4 Inferences Concerning a Difference Between Population Proportions", " Ch. 9 Inferences Based on Two Samples Sections covered: 9.1, 9.2, 9.3, 9.4 9.1 \\(z\\) Tests and CI’s for a Difference Between Two Population Means (Cases 1 &amp; 2) Skip: \\(\\beta\\) and the Choice of Sample Size (pp. 366-367) R Since you aren’t given the original data, R isn’t very helpful here, but you can write a function that you could reuse, such as: options(scipen = 999) # get rid of scientific notation # this function only has to be run once per session, and then you can reuse it. diffmeans &lt;- function(xbar, ybar, delta0, sigma1, sigma2, m, n, type = &quot;twosided&quot;) { Z &lt;- ((xbar - ybar) - delta0)/sqrt(((sigma1^2)/m) + ((sigma2^2)/n)) if (type == &quot;twosided&quot;) { pvalue &lt;- pnorm(-abs(Z))*2 } else if (type == &quot;lowertail&quot;) { pvalue &lt;- pnorm(Z) } else { pvalue &lt;- 1 - pnorm(Z) } print(c(&quot;The p-value is&quot;, round(pvalue, 4))) } # Example 9.1, p. 365 diffmeans(xbar = 29.8, ybar = 34.7, delta0 = 0, sigma1 = 4, sigma2 = 5, m = 20, n = 25, type = &quot;twosided&quot;) ## [1] &quot;The p-value is&quot; &quot;0.0003&quot; # Example 9.4, p. 368 # As long as you use the correct order of parameters, you don&#39;t have to write out the names: diffmeans(2258, 2637, -200, 1519, 1138, 663, 413, &quot;lowertail&quot;) ## [1] &quot;The p-value is&quot; &quot;0.0139&quot; 9.2 The Two-Sample \\(t\\) Test and CI (Case 3) Use: http://bit.ly/degreesoffreedom to calculate the degrees of freedom Skip: Pooled \\(t\\) Procedures (pp. 377-378) Skip: Type II Error Probabilities (pp. 378-379) 9.3 Analysis of Paired Data Skip: Paired Data and Two-Sample \\(t\\) Procedures (pp. 386-387) Skip: Paired Versus Unpaired Experiments (pp. 387-388) 9.4 Inferences Concerning a Difference Between Population Proportions Skip: Type II Error Probabilities and Sample Sizes (pp. 394-395) R A/B Testing question from class: clicks &lt;- c(25, 20) people &lt;- c(100, 100) prop.test(clicks, people, correct = FALSE) ## ## 2-sample test for equality of proportions without continuity ## correction ## ## data: clicks out of people ## X-squared = 0.71685, df = 1, p-value = 0.3972 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## -0.06553817 0.16553817 ## sample estimates: ## prop 1 prop 2 ## 0.25 0.20 "],
["ch-2-conditional-probability.html", "Ch. 2 Conditional Probability Resources", " Ch. 2 Conditional Probability Sections covered: 2.4, 2.5 (Section 2.5 was covered earlier; now we’re adding the definition of independence in terms of conditional probability on p. 85: “Two events A and B are independent if P(A|B) = P(A) and are dependent otherwise.”) Resources Extra practice problems, Bayes Theorem (Posted on Twitter as a response to Bill Gates’ provocative comment that he’d rather encounter a shark than a mosquito in the wild.) An Intuitive (and Short) Explanation of Bayes’ Theorem "],
["ch-5-joint-probability.html", "Ch. 5 Joint Probability 5.1 Jointly Distributed Random Variables 5.2 Expected Values, Covariance, and Correlation Resources", " Ch. 5 Joint Probability Sections covered: 5.1, 5.2 5.1 Jointly Distributed Random Variables Skip everything but “Two Discrete Random Variables” pp. 199-200 5.2 Expected Values, Covariance, and Correlation Skip everything but “Covariance” pp. 214-215 and “Correlation” pp. 216-218 (In both sections, skip double integrals) Resources Correlation and Covariance Visualization http://shiny.albany.edu/stat/rectangles/ "],
["ch-14-chi-squared-test.html", "Ch. 14 Chi Squared Test 14.3 Two-Way Contingency Tables Resources R", " Ch. 14 Chi Squared Test Sections covered: 14.3 14.3 Two-Way Contingency Tables Skip: pp. 639-643, including “Testing for Homogeneity” Focus on “Testing for Independence (Lack of Association)” Notes on the chi square test formula on p. 644: Write the null hypothesis as a sentence, not as in the book. (For example: “Class and Survival Status are independent.”) “estimated expected” in the textbook is the same as “expected” used in class I and J refer to the number of rows and columns in the table Resources Chi Square Table Chi Squared Test Calculator Chi Squared Distribution Curves R The chisq.test() function requires that data be in matrix form: # p. 647, #28 mat &lt;- matrix(c(28, 17, 7, 31, 26, 10, 26, 19, 11), nrow = 3, byrow = TRUE) dimnames(mat) &lt;- list(`Email_Provider` = c(&quot;gmail&quot;, &quot;Yahoo&quot;, &quot;Other&quot;), `Cell_Phone_Provider` = c(&quot;ATT&quot;, &quot;Verizon&quot;, &quot;Other&quot;)) chisq.test(mat, correct = FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: mat ## X-squared = 1.5074, df = 4, p-value = 0.8253 To see the expected values: results &lt;- chisq.test(mat, correct = FALSE) round(results$expected, 2) ## Cell_Phone_Provider ## Email_Provider ATT Verizon Other ## gmail 25.26 18.42 8.32 ## Yahoo 32.54 23.74 10.72 ## Other 27.20 19.84 8.96 Mosaic plot mosaicplot(t(mat), color = c(&quot;aliceblue&quot;, &quot;cornflowerblue&quot;, &quot;navyblue&quot;), main = &quot;&quot;) See this tutorial for more on mosaic plots. "],
["ch-12-linear-regression.html", "Ch. 12 Linear Regression 12.1 The Simple Linear Regression Model 12.2 Estimating Model Parameters 12.5 Correlation", " Ch. 12 Linear Regression Sections covered: 12.1, 12.2, 12.5 12.1 The Simple Linear Regression Model 12.2 Estimating Model Parameters Resources Interactive Visualization: Linear Regression Try fitting the least squares line to a set of random data and check your answer (and another one). Video: Regression I: What is regression? | SSE, SSR, SST | R-squared | Errors (ε vs. e) [contributed by Lance J.] R Calculating slope and intercept for a sample of (x, y) pairs (p. 498 formulas) # Example 12.8, p. 503 x &lt;- c(12, 30, 36, 40, 45, 57, 62, 67, 71, 78, 93, 94, 100, 105) y &lt;- c(3.3, 3.2, 3.4, 3, 2.8, 2.9, 2.7, 2.6, 2.5, 2.6, 2.2, 2, 2.3, 2.1) lm(y ~ x) #lm = linear model ## ## Call: ## lm(formula = y ~ x) ## ## Coefficients: ## (Intercept) x ## 3.62091 -0.01471 Predicted values: mod &lt;- lm(y~x) round(mod$fitted.values, 2) ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## 3.44 3.18 3.09 3.03 2.96 2.78 2.71 2.64 2.58 2.47 2.25 2.24 2.15 2.08 Residuals: round(mod$residuals, 2) ## 1 2 3 4 5 6 7 8 9 10 11 12 ## -0.14 0.02 0.31 -0.03 -0.16 0.12 -0.01 -0.04 -0.08 0.13 -0.05 -0.24 ## 13 14 ## 0.15 0.02 SSE, SSR anova(mod) # anova = analysis of variance ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 1 2.29469 2.29469 104.92 0.0000002762 *** ## Residuals 12 0.26246 0.02187 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 SSE = 0.2624565 SSR = 2.2946864 SST = SSE + SSR = 0.2624565 + 2.2946864 = 2.5571 coefficient of determination \\(r^2\\) # Example 12.4, 12.9 x &lt;- c(132, 129, 120, 113.2, 105, 92, 84, 83.2, 88.4, 59, 80, 81.5, 71, 69.2) y &lt;- c(46, 48, 51, 52.1, 54, 52, 59, 58.7, 61.6, 64, 61.4, 54.6, 58.8, 58) mod &lt;- lm(y ~ x) SSE &lt;- anova(mod)[2,2] SST &lt;- anova(mod)[1,2] + anova(mod)[2,2] 1 - (SSE/SST) ## [1] 0.7907602 Or (simply): cor(x,y)^2 ## [1] 0.7907602 (See section 12.5) 12.5 Correlation Skip: “Inferences About the Population Correlation Coefficient” (p. 530) to end of section. Resources Interactive visualization: Correlation Coefficient (add and remove points) Interactive visualization: Interpreting Correlations [contributed by Dario G.] R sample correlation coefficient \\(r\\) # Example 12.15, p. 528 x &lt;- c(2.4, 3.4, 4.6, 3.7, 2.2, 3.3, 4.0, 2.1) y &lt;- c(1.33, 2.12, 1.80, 1.65, 2.00, 1.76, 2.11, 1.63) cor(x,y) ## [1] 0.3472602 "],
["ch-13-nonlinear-and-multiple-regression.html", "Ch. 13 Nonlinear and Multiple Regression 13.1 Assessing Model Adequacy", " Ch. 13 Nonlinear and Multiple Regression Sections covered: 13.1 13.1 Assessing Model Adequacy Skip: standardized residuals Residuals plots: know \\(e\\) vs. \\(x\\) only Difficulties and remedies: 1. - 4. – know the difficulties, not the remedies Skip: weighted least squares (p. 547) "]
]
